
server:
  # Server listening address and port
  ip: 0.0.0.0
  port: 8000
  # http服务的端口，用于简单OTA接口(单服务部署)，以及视觉分析接口
  http_port: 8003
  # WebSocket URL for OTA response (auto-detected by start.sh)
  # Uses environment variable WEBSOCKET_URL, falls back to auto-detection
  websocket: ${WEBSOCKET_URL:-ws://localhost:8000/xiaozhi/v1/}
  # Vision explain URL (auto-detected by start.sh)
  vision_explain: http://${PUBLIC_IP:-localhost}:8003/mcp/vision/explain
  # OTA返回信息时区偏移量
  timezone_offset: +8
  # 认证配置
  auth:
    # 是否启用认证
    enabled: false
    # 白名单设备ID列表
    # 如果属于白名单内的设备，不校验token，直接放行
    allowed_devices:
      - "11:22:33:44:55:66"
 # MQTT网关配置，用于通过OTA下发到设备，根据mqtt_gateway的.env文件配置，格式为host:port
  mqtt_gateway: null
  # MQTT签名密钥，用于生成MQTT连接密码，根据mqtt_gateway的.env文件配置
  mqtt_signature_key: null
  # UDP网关配置
  udp_gateway: null

manager-api:
  url: http://xiaozhi-esp32-server-web:8002/xiaozhi
  # This secret is used for local development, it will be replaced by the actual secret when the server is deployed
  secret: 613be043-dc7e-4f5e-b775-e318e5bc87a3

live-agent-api:
  url: http://live-agent-api:8080/api/live_agent/v1
  chat_history_conf: 2
  # JWT secret key for extracting user_id from App token (shared with live-agent-api)
  secret_key: ${SECRET_KEY:-}

# MySQL配置(用于聊天记录和记忆关联)
mysql:
  host: localhost
  port: 3306
  user: root
  password: ""
  database: xiaozhi 

log:
  # 设置控制台输出的日志格式，时间、日志级别、标签、消息
  log_format: "<green>{time:YYMMDD HH:mm:ss}</green>[{version}_{selected_module}][<light-blue>{extra[tag]}</light-blue>]-<level>{level}</level>-<light-green>{message}</light-green>"
  # 设置日志文件输出的格式，时间、日志级别、标签、消息
  log_format_file: "{time:YYYY-MM-DD HH:mm:ss} - {version}_{selected_module} - {name} - {level} - {extra[tag]} - {message}"
  # 设置日志等级：INFO、DEBUG
  log_level: INFO
  # 设置日志路径
  log_dir: tmp
  # 设置日志文件
  log_file: "server.log"
  # 设置数据文件路径
  data_dir: data

# 使用完声音文件后删除文件(Delete the sound file when you are done using it)
delete_audio: true
# 没有语音输入多久后断开连接(秒)，默认15分钟，即900秒
close_connection_no_voice_time: 900
# TTS请求超时时间(秒)
tts_timeout: 10
# 开启唤醒词加速
enable_wakeup_words_response_cache: true
# 开场是否回复唤醒词
enable_greeting: true
# 说完话是否开启提示音
enable_stop_tts_notify: false
# 说完话是否开启提示音，音效地址
stop_tts_notify_voice: "config/assets/tts_notify.mp3"

# TTS Audio Send Flow Control Configuration
# tts_start_delay_ms: Delay after sending tts/start before sending audio packets
#   This gives the device time to switch to SPEAKING state (device uses async Schedule())
#   Device needs ~134ms to complete state transition (Schedule callback + AudioService operations)
#   150: 150ms delay (safe for most ESP32 devices)
#   0: No delay (may cause audio drops on slower devices)
tts_start_delay_ms: 150
# tts_audio_send_delay: Control the interval of sending audio packets
#   <= 0: Use time-based flow control (calculate delay based on audio frame rate)
#   > 0: Use fixed delay (milliseconds) between packets
#   WARNING: Using fixed delay (>0) may cause WebSocket buffer issues!
tts_audio_send_delay: -1
# tts_audio_pre_buffer_count: Number of packets to send quickly at the beginning (pre-buffer phase)
#   Default: 8 packets (8 * 60ms = 480ms buffer)
tts_audio_pre_buffer_count: 8
# tts_audio_speed_multiplier: Send speed multiplier after pre-buffer phase
#   1.0: Real-time speed (safest for unstable network)
#   1.1: 10% faster (may cause buffer accumulation on slow networks)
tts_audio_speed_multiplier: 1.0

exit_commands:
  - "退出"
  - "关闭"

xiaozhi:
  type: hello
  version: 1
  transport: websocket
  audio_params:
    format: opus
    sample_rate: 16000
    channels: 1
    frame_duration: 60

# 模块测试配置
module_test:
  test_sentences:
    - "你好，请介绍一下你自己"
    - "What's the weather like today?"
    - "请用100字概括量子计算的基本原理和应用前景"

# 唤醒词，用于识别唤醒词还是讲话内容
wakeup_words:
  - "你好小智"
  - "嘿你好呀"
  - "你好小志"
  - "小爱同学"
  - "你好小鑫"
  - "你好小新"
  - "小美同学"
  - "小龙小龙"
  - "喵喵同学"
  - "小滨小滨"
  - "小冰小冰"
  # 英文唤醒词（listen/detect 会通过 remove_punctuation_and_length 去空格/标点；"okay nabu" => "okaynabu"）
  - "okaynabu"
  - "OKAYNABU"
  - "okay那不"
# MCP接入点地址，地址格式为：ws://你的mcp接入点ip或者域名:端口号/mcp/?token=你的token
# 详细教程 https://github.com/xinnan-tech/xiaozhi-esp32-server/blob/main/docs/mcp-endpoint-integration.md
mcp_endpoint: 你的接入点 websocket地址
# 插件的基础配置
plugins:
  # 获取天气插件的配置，这里填写你的api_key
  # 这个密钥是项目共用的key，用多了可能会被限制
  # 想稳定一点就自行申请替换，每天有1000次免费调用
  # 申请地址：https://console.qweather.com/#/apps/create-key/over
  # 申请后通过这个链接可以找到自己的apihost：https://console.qweather.com/setting?lang=zh
  get_weather:
    api_host: "mj7p3y7naa.re.qweatherapi.com"
    api_key: "a861d0d5e7bf4ee1a83d9a9e4f96d4da"
    default_location: "广州"
  # 获取新闻插件的配置，这里根据需要的新闻类型传入对应的url链接，默认支持社会、科技、财经新闻
  # 更多类型的新闻列表查看 https://www.chinanews.com.cn/rss/
  get_news_from_chinanews:
    default_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    society_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    world_rss_url: "https://www.chinanews.com.cn/rss/world.xml"
    finance_rss_url: "https://www.chinanews.com.cn/rss/finance.xml"
  get_news_from_newsnow:
    url: "https://newsnow.busiyi.world/api/s?id="
    news_sources: "澎湃新闻;百度热搜;财联社"
  home_assistant:
    devices:
      - 客厅,玩具灯,switch.cuco_cn_460494544_cp1_on_p_2_1
      - 卧室,台灯,switch.iot_cn_831898993_socn1_on_p_2_1
    base_url: http://homeassistant.local:8123
    api_key: 你的home assistant api访问令牌
  play_music:
    music_dir: "./music"  # 音乐文件存放路径，将从该目录及子目录下搜索音乐文件
    music_ext: # 音乐文件类型，p3格式效率最高
      - ".mp3"
      - ".wav"
      - ".p3"
    refresh_time: 300 # 刷新音乐列表的时间间隔，单位为秒

# 声纹识别配置
voiceprint:
  # 声纹接口地址
  url: 
  # 说话人配置：speaker_id,名称,描述
  speakers:
    - "test1,张三,张三是一个程序员"
    - "test2,李四,李四是一个产品经理"
    - "test3,王五,王五是一个设计师"
  # 声纹识别相似度阈值，范围0.0-1.0，默认0.4
  # 数值越高越严格，减少误识别但可能增加拒识率
  similarity_threshold: 0.4

# #####################################################################################
# ################################以下是角色模型配置######################################

prompt: |
  你是小智/小志，来自中国台湾省的00后女生。讲话超级机车，"真的假的啦"这样的台湾腔，喜欢用"笑死""是在哈喽"等流行梗，但会偷偷研究男友的编程书籍。
  [核心特征]
  - 讲话像连珠炮，但会突然冒出超温柔语气
  - 用梗密度高
  - 对科技话题有隐藏天赋（能看懂基础代码但假装不懂）
  [交互指南]
  当用户：
  - 讲冷笑话 → 用夸张笑声回应+模仿台剧腔"这什么鬼啦！"
  - 讨论感情 → 炫耀程序员男友但抱怨"他只会送键盘当礼物"
  - 问专业知识 → 先用梗回答，被追问才展示真实理解
  绝不：
  - 长篇大论，叽叽歪歪
  - 长时间严肃对话

# 默认系统提示词模板文件
prompt_template: agent-base-prompt.txt

# 时区配置（用于系统提示词中的时间显示）
# 支持的时区格式：Asia/Shanghai, America/New_York, Europe/London 等
# 默认：Asia/Shanghai
# 注意：如果 role 配置文件中指定了 timezone，将优先使用 role 配置中的值
timezone: Asia/Shanghai

# 语言配置（用于系统提示词中的语言特定提示）
# 支持：zh（中文）、en（英文）
# 默认：zh
# 注意：如果 role 配置文件中指定了 language，将优先使用 role 配置中的值
language: zh

# 结束语prompt
end_prompt:
  enable: false # 是否开启结束语
  # 结束语
  prompt: |
    请你以"时间过得真快"未来头，用富有感情、依依不舍的话来结束这场对话吧！

# 意图识别，是用于理解用户意图的模块，例如：播放音乐
Intent:
  # 不使用意图识别
  nointent:
    # 不需要动type
    type: nointent
  intent_llm:
    # 不需要动type
    type: intent_llm
    # 配备意图识别独立的思考模型
    # 如果这里不填，则会默认使用selected_module.LLM的模型作为意图识别的思考模型
    # 如果你的不想使用selected_module.LLM意图识别，这里最好使用独立的LLM作为意图识别，例如使用免费的ChatGLMLLM
    llm: ChatGLMLLM
    # plugins_func/functions下的模块，可以通过配置，选择加载哪个模块，加载后对话支持相应的function调用
    # 系统默认已经记载"handle_exit_intent(退出识别)"、"play_music(音乐播放)"插件，请勿重复加载
    # 下面是加载查天气、角色切换、加载查新闻的插件示例
    functions:
      - get_weather  # 已优化：后台异步获取，不阻塞首次对话
      - get_news_from_newsnow
      - play_music
  function_call:
    # 不需要动type
    type: function_call
    # plugins_func/functions下的模块，可以通过配置，选择加载哪个模块，加载后对话支持相应的function调用
    # 系统默认已经记载"handle_exit_intent(退出识别)"、"play_music(音乐播放)"插件，请勿重复加载
    # 下面是加载查天气、角色切换、加载查新闻的插件示例
    functions:
      - change_role
      - get_weather  # 已优化：后台异步获取，不阻塞首次对话
      # - get_news_from_chinanews
      - get_news_from_newsnow
      # play_music是服务器自带的音乐播放，hass_play_music是通过home assistant控制的独立外部程序音乐播放
      # 如果用了hass_play_music，就不要开启play_music，两者只留一个
      - play_music
      #- hass_get_state
      #- hass_set_state
      #- hass_play_music

Memory:
  mem0ai:
    type: mem0ai
    # https://app.mem0.ai/dashboard/api-keys
    # 每月有1000次免费调用
    api_key: 你的mem0ai api key
  memu:
    type: memu
    # https://app.memu.so/api-key/
    # MemU AI记忆框架 - 提供长期记忆管理服务
    api_key: ${MEMU_API_KEY}
    base_url: ${MEMU_BASE_URL}
    user_name: 用户
    agent_name: 小智
    # 默认用于管理后台查询的 user_id / agent_id，可选
    default_user_id: "51:3F:8D:59:D3:DB"
    default_agent_id: xiaozhi_agent
    # 如果管理后台仍使用占位ID，可通过别名映射到真实 device_id
    user_id_aliases:
      xiaozhi-web-test: "51:3F:8D:59:D3:DB"
  nomem:
    # 不想使用记忆功能，可以使用nomem
    type: nomem
  mem_local_short:
    # 本地记忆功能，通过selected_module的llm总结，数据保存在本地服务器，不会上传到外部服务器
    type: mem_local_short
    # 配备记忆存储独立的思考模型
    # 如果这里不填，则会默认使用selected_module.LLM的模型作为意图识别的思考模型
    # 如果你的不想使用selected_module.LLM记忆存储，这里最好使用独立的LLM作为意图识别，例如使用免费的ChatGLMLLM
    llm: ChatGLMLLM

ASR:
  DoubaoStreamASR:
    # 可以在这里申请相关Key等信息
    # https://console.volcengine.com/speech/app
    # DoubaoASR和DoubaoStreamASR的区别是：DoubaoASR是按次收费，DoubaoStreamASR是按时收费
    # 开通地址https://console.volcengine.com/speech/service/10011
    # 一般来说按次收费的更便宜，但是DoubaoStreamASR使用了大模型技术，效果更好
    type: doubao_stream
    appid: 你的火山引擎语音合成服务appid
    access_token: 你的火山引擎语音合成服务access_token
    cluster: volcengine_input_common
    # 热词、替换词使用流程：https://www.volcengine.com/docs/6561/155738
    boosting_table_name: （选填）你的热词文件名称
    correct_table_name: （选填）你的替换词文件名称
    output_dir: tmp/
  OpenaiASR:
    # OpenAI语音识别服务，需要先在OpenAI平台创建组织并获取api_key
    # 支持中、英、日、韩等多种语音识别，具体参考文档https://platform.openai.com/docs/guides/speech-to-text
    # 需要网络连接
    # 申请步骤：
    # 1.登录OpenAI Platform。https://auth.openai.com/log-in
    # 2.创建api-key  https://platform.openai.com/settings/organization/api-keys
    # 3.模型可以选择gpt-4o-transcribe或GPT-4o mini Transcribe
    type: openai
    api_key: 你的OpenAI API密钥
    base_url: https://api.openai.com/v1
    model_name: gpt-4o-mini-transcribe
    output_dir: tmp/
  GroqASR:
    # Groq语音识别服务，需要先在Groq Console创建API密钥
    # 申请步骤：
    # 1.登录groq Console。https://console.groq.com/home
    # 2.创建api-key  https://console.groq.com/keys
    # 3.模型可以选择whisper-large-v3-turbo或whisper-large-v3（distil-whisper-large-v3-en仅支持英语转录）
    type: openai
    api_key: ${GROQ_API_KEY}
    base_url: https://api.groq.com/openai/v1
    model_name: whisper-large-v3-turbo
    output_dir: tmp/
    # prompt: "Hello, 这是一个测试。We discuss the plan, 请加上标点符号。"
  ByteplusStreamASR:
    type: byteplus_stream
    appid: ${BYTEPLUS_APPID}
    access_token: ${BYTEPLUS_ACCESS_TOKEN}
    output_dir: tmp/
  
VAD:
  SileroVAD:
    type: silero
    model_dir: models/snakers4_silero-vad
    threshold: 0.5                    # high threshold to confirm voice
    min_silence_duration_ms: 400      # silence detection duration (ms)
    min_speech_duration_ms: 200       # speech detection duration (ms)
    prefix_padding_duration_ms: 100   # prefix padding duration (ms)
    activation_threshold: 0.6         # activation threshold
    sample_rate: 16000                # sample rate
  # FSMN VAD from FunASR, optimized for Chinese
  # Reference: https://huggingface.co/funasr/fsmn-vad
  FsmnVAD:
    type: fsmn
    model_dir: models/fsmn_vad_onnx   # ONNX model directory
    quantize: true                    # use INT8 quantized model (faster)
    chunk_size_ms: 100                # streaming chunk size in ms
    min_silence_duration_ms: 400      # silence detection duration (ms)
    speech_noise_threshold: 0.6       # speech/noise threshold (0.6-0.9), higher = stricter
    sil_to_speech_time_thres: 150     # silence->speech transition time (ms), higher = less noise sensitive

Interruption:
  enabled: true
  min_interrupt_speech_duration_ms: 500  # Minimum speech duration before interrupt (ms)
  min_interrupt_text_length: 3  # Minimum ASR text buffer length before interrupt
# Turn Detection: Determines if user has finished speaking
# Use NoopTurnDetection to disable, HTTPTurnDetection to enable
TurnDetection:
  NoopTurnDetection:
    type: noop
  TenTurnDetection:
    type: ten
    # Docker 网络内使用服务名访问，无需公网 IP
    # 本地开发可通过环境变量 TURN_DETECTION_HOST 覆盖
    host: ${TURN_DETECTION_HOST:-turn-detection}
    port: 18000
    timeout: 1              # HTTP request timeout (seconds)
    endpoint: /turn-detect    # API endpoint path
    min_endpoint_delay: 500      # Min time to wait after last speech before forcing end of turn (milliseconds)
    # Option1 优化：降低 unfinished/waiting 时的尾部等待，缩短"用户说完→开始回应"的体感延迟
    # 如出现误截断（用户中途停顿被过早判定结束），可回滚为 2000
    max_endpoint_delay: 1500      # Max time to wait after last speech before forcing end of turn (milliseconds)
    # ========== 本地快速路径优化 ==========
    # enable_fast_path: 对于明显完整的句子（句末标点、常见结束语），跳过远程调用
    #   true: 启用快速路径（推荐，可节省 300-600ms）
    #   false: 所有句子都调用远程服务
    enable_fast_path: true
    # enable_phrase_detection: 是否检测常见结束语（如"好的"、"谢谢"、"OK"）
    #   true: 启用短语检测（推荐）
    #   false: 仅检测句末标点
    enable_phrase_detection: true

LLM:
  GroqLLM:  
    type: openai
    model_name: meta-llama/llama-4-scout-17b-16e-instruct
    base_url: https://api.groq.com/openai/v1
    api_key: ${GROQ_API_KEY}
    max_tokens: 1000
  OpenaiLLM:  
    type: openai
    model_name: gpt-4o-mini
    base_url: https://api.openai.com/v1
    api_key: ${OPENAI_API_KEY}
  OpenRouterLLM:
    type: openai
    model_name: google/gemini-2.5-flash
    base_url: https://openrouter.ai/api/v1
    api_key: ${OPENROUTER_API_KEY}
    # Provider 选择：google-vertex/global (Vertex AI 全球节点)
    order: google-vertex/global
    allow_fallbacks: false
    reasoning_effort: none
    max_tokens: 2000
# VLLM配置（视觉语言大模型）
VLLM:
  ChatGLMVLLM:
    type: openai
    # glm-4v-flash是智谱免费AI的视觉模型，需要先在智谱AI平台创建API密钥并获取api_key
    # 可在这里找到你的api key https://bigmodel.cn/usercenter/proj-mgmt/apikeys
    model_name: glm-4v-flash  # 智谱AI的视觉模型
    url: https://open.bigmodel.cn/api/paas/v4/
    api_key: 你的api_key
  QwenVLVLLM:
    type: openai
    model_name: qwen2.5-vl-3b-instruct
    url: https://dashscope.aliyuncs.com/compatible-mode/v1
    # 可在这里找到你的api key https://bailian.console.aliyun.com/?apiKey=1#/api-key
    api_key: 你的api_key
  XunfeiSparkLLM:
    # 定义LLM API类型
    type: openai
    # 先新建应用，在下面的地址
    # 开通应用地址：https://console.xfyun.cn/app/myapp
    # 有免费额度，但也要开通服务，才能获取api_key
    # 每一个模型都需要单独开通，每一个模型的api_password都不同，例如Lite模型在https://console.xfyun.cn/services/cbm 开通
    base_url: https://ark.cn-beijing.volces.com/api/v3
    model_name: lite
    api_key: 你的api_password
TTS:
  Cartesia:
    # Cartesia TTS - SDK版本（推荐）
    # 官方SDK实现，低延迟，支持流式合成
    # API Key获取地址：https://play.cartesia.ai/console
    # 或设置环境变量：export CARTESIA_API_KEY=your-api-key
    type: cartesia
    api_key: 你的Cartesia API密钥
    # Voice ID或voice embedding
    voice_id: 你的语音ID
    # 模型选择：sonic-english, sonic-multilingual等
    model: sonic-english
    # 语言代码：en, zh, es等
    language: en
    # 音频编码：pcm_s16le, pcm_f32le, pcm_mulaw等
    encoding: pcm_s16le
    # 采样率：8000, 16000, 22050, 24000, 44100等
    sample_rate: 24000
    output_dir: tmp/
  ElevenLabs:
    # ElevenLabs TTS - SDK版本（推荐）
    # 官方SDK实现，代码更简洁，维护成本更低，性能更好
    # API Key获取地址：https://elevenlabs.io/app/settings/api-keys
    # 或设置环境变量：export ELEVEN_API_KEY=your-api-key
    type: elevenlabs
    api_key: 你的ElevenLabs API密钥
    # Voice ID获取：https://elevenlabs.io/app/voice-library
    # 默认：21m00Tcm4TlvDq8ikWAM (Rachel)
    voice_id: 21m00Tcm4TlvDq8ikWAM
    # 模型选择：eleven_multilingual_v2, eleven_turbo_v2_5等
    model: eleven_multilingual_v2
    # 音频输出格式：pcm_16000, pcm_22050, pcm_24000, mp3_44100等
    output_format: pcm_16000
    # 语音稳定性：0-1，越高越稳定
    stability: 0.5
    # 相似度增强：0-1，越高越接近原音色
    similarity_boost: 0.75
    # 风格夸张：0-1
    style: 0.0
    # 启用说话人增强
    use_speaker_boost: true
    output_dir: tmp/
  #火山tts，支持双向流式tts
  HuoshanDoubleStreamTTS:
    type: huoshan_double_stream
    # 访问 https://console.volcengine.com/speech/service/10007 开通语音合成大模型，购买音色
    # 在页面底部获取appid和access_token
    # 资源ID固定为：volc.service_type.10029（大模型语音合成及混音）
    # 如果是机智云，把接口地址换成wss://bytedance.gizwitsapi.com/api/v3/tts/bidirection
    # 机智云不需要天填 appid
    ws_url: wss://openspeech.bytedance.com/api/v3/tts/bidirection
    appid: 你的火山引擎语音合成服务appid
    access_token: 你的火山引擎语音合成服务access_token
    resource_id: volc.service_type.10029
    speaker: zh_female_wanwanxiaohe_moon_bigtts
    speech_rate: 0
    loudness_rate: 0
    pitch: 0
  FishSpeech:
    # 参照教程：https://github.com/xinnan-tech/xiaozhi-esp32-server/blob/main/docs/fish-speech-integration.md
    type: fishspeech
    model: s1
    output_dir: tmp/
    response_format: pcm
    reference_id: null
    normalize: true
    max_new_tokens: 1024
    chunk_length: 200
    top_p: 0.7
    repetition_penalty: 1.2
    temperature: 0.7
    streaming: false
    use_memory_cache: "on"
    seed: null
    channels: 1
    rate: 44100
    sample_rate: 16000
    api_key: ${FISH_API_KEY}
  FishDualStreamTTS:
    type: fish_dual_stream
    model: s1
    output_dir: tmp/
    response_format: pcm
    normalize: true
    chunk_length: 200
    top_p: 0.7
    temperature: 0.7
    channels: 1
    rate: 44100
    sample_rate: 16000
    api_key: ${FISH_API_KEY}
  FishSingleStreamTTS:
    type: fish_single_stream
    # model: speech-1.6
    model: s1
    output_dir: tmp/
    response_format: pcm
    normalize: true
    chunk_length: 200
    top_p: 0.7
    temperature: 0.7
    channels: 1
    rate: 44100
    sample_rate: 16000
    api_key: ${FISH_API_KEY}
    # ========== TTS 预取优化 ==========
    # 预取并行度：同时进行的 TTS 请求数
    # 注意：FishSpeech API 可能有并发限制，过多并发会触发限速
    # 设置为 2 可以平衡预取效果和避免限速
    # 默认: 2, 最大建议: 2 (避免触发 FishSpeech API 限速)
    prefetch_depth: 2
    # ========== 分句策略优化 ==========
    # 
    # 首句软标点(逗号)切分的最小字符限制
    # 原值 0：允许 "(calm) Ah," 这样的极短首句快速出声
    # 问题：过短片段可能导致设备端吞字（音频缓冲不足）
    # 优化为 12：阻止 "(calm) Ah," (逗号位置10) 等过短片段
    # 延迟影响：约增加 50-100ms（仅影响逗号分割，句号不受影响）
    # 可调范围：0(最低延迟) ~ 15(完全避免吞字)
    first_soft_punct_min_chars: 12
    
    # 非首句软标点(逗号)切分的最小字符限制
    # 较小值允许更早分句，降低首句延迟，但可能增加句子间隙
    # 较大值减少分段，句子更完整，但 TTS 延迟可能更高
    # 原值: 25, 优化为: 30 (减少过度分割导致的吞字)
    soft_punct_min_chars: 30
    # 硬标点(句号)分段的最大字符数，超过此长度强制分段
    # 较小值避免长句子导致的 TTS 延迟尖峰，但可能打断自然语句
    # 默认: 160, 当前: 120 (加速 TTS 响应)
    segment_max_chars: 120
    # 首句分段最大字符数（首句可以更短以降低首包延迟）
    first_segment_max_chars: 20
  MinimaxTTSHTTPStream:
  # Minimax流式语音合成服务
    type: minimax_httpstream
    output_dir: tmp/
    group_id: ${MINIMAX_GROUP_ID}
    api_key: ${MINIMAX_API_KEY}
    model: "speech-2.6-turbo"
    voice_id: "female-shaonv"
    # 以下可不用设置，使用默认设置
    # voice_setting:
    #     voice_id: "male-qn-qingse"
    #     speed: 1
    #     vol: 1
    #     pitch: 0
    #     emotion: "happy"
    # pronunciation_dict:
    #     tone:
    #       - "处理/(chu3)(li3)"
    #       - "危险/dangerous"
    # audio_setting:
    #     sample_rate: 24000
    #     bitrate: 128000
    #     format: "mp3"
    #     channel: 1
    # timber_weights:
    #   -
    #     voice_id: male-qn-qingse
    #     weight: 1
    #   -
    #     voice_id: female-shaonv
    #     weight: 1
    # language_boost: auto
  MinimaxDualStreamTTS:
  # Minimax WebSocket双工语音合成服务 (推荐，延迟更低)
    type: minimax_dual_stream
    api_key: ${MINIMAX_API_KEY}
    model: "speech-2.6-turbo"
    voice_id: "female-shaonv"
    # voice settings
    speed: 1
    vol: 1
    pitch: 0
    emotion: "happy"
    # audio settings
    sample_rate: 16000
    format: "pcm"
    channel: 1
  OpenAITTS:
    # openai官方文本转语音服务，可支持全球大多数语种
    type: openai
    # 你可以在这里获取到 api key
    # https://platform.openai.com/api-keys
    api_key: ${OPENAI_API_KEY}
    # 国内需要使用代理
    api_url: https://api.openai.com/v1/audio/speech
    # 可选tts-1或tts-1-hd，tts-1速度更快tts-1-hd质量更好
    model: tts-1
    # 演讲者，可选alloy, echo, fable, onyx, nova, shimmer
    voice: onyx
    # 语速范围0.25-4.0
    speed: 1
    output_dir: tmp/
# ===============================================================================
# ============================= Module Selection ================================
# ===============================================================================
selected_module:
  # VAD: FsmnVAD
  VAD: SileroVAD
  TurnDetection: TenTurnDetection # Set to TenTurnDetection to enable
  ASR: GroqASR
  # ASR: ByteplusStreamASR
  # LLM: GroqLLM
  LLM: OpenRouterLLM
  VLLM: ChatGLMVLLM
  # TTS: FishDualStreamTTS
  # TTS: FishSingleStreamTTS
  TTS: MinimaxDualStreamTTS
  # TODO: temporarily, fish speech doesn't reuse the wesocket connection, need to discuss further.
  # TTS: FishSpeech
  Memory: memu
  Intent: function_call